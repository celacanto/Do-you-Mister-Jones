x$Year <- year
return(x)
}
years <- 2013:1946
total <- length(years)
pb <- txtProgressBar(min = 0, max = total, style = 3)
cannesList <- list()
for(y in seq_len(total)){
cannesList[[y]] <- getCannesSelection(years[y])
setTxtProgressBar(pb, y)
}
close(pb)
y
years[y]
selectionsLists
length(selectionsLists)
library(XML)
library(stringr)
getCannesSelection <- function(year){
cannesUrl <- paste("http://www.festival-cannes.com/en/archives/", year, "/allSelections.html", sep = "")
canesHtml <- htmlParse(cannesUrl, encoding = "utf-8")
selectionsListsRaw <- readHTMLList(canesHtml)
selectionsLists <- head(selectionsListsRaw[-(1:8)], -1)
if(length(selectionsLists) == 0) {
# In 1950 and 1948 there were no Festival
return("festival was not this year")
}
titleRaw <- lapply(selectionsLists, str_extract,  "^.*?\t")
titleClean <- lapply(titleRaw, gsub, pattern = " ?\t", replacement = "")
orName <- lapply(titleClean, gsub, pattern = " ?\\(.*\\)( with .*)?$", replacement = "")
engNameRaw <- lapply(titleClean, str_extract, pattern = " ?\\(.*\\)( with .*)?$")
engNameTrim <- lapply(engNameRaw, str_trim)
engName <- lapply(engNameTrim, gsub, pattern = "\\(|\\)| with .*$", replacement = "")
directorRaw <- lapply(selectionsLists, str_extract,  "\t .*")
directorWithand <- lapply(directorRaw, gsub, pattern = ", \t{2,}", replacement = " / ")
director <- lapply(directorWithand, gsub, pattern = "\t directed by ", replacement = "")
selectionTypeRaw <- xpathSApply(canesHtml, "//h2", xmlValue)
# There are some <h2> headers after the selections names.
# All theses shit <h2> headers are after the "select a year"
selectAyearIndex <- grep("Select a year", selectionTypeRaw)
selectionType <- selectionTypeRaw[1:(selectAyearIndex - 1)]
selectionSize <- sapply(director, length)
selection <- mapply(rep, x = selectionType, times = selectionSize)
listVar <- list(selection, orName, engName, director)
x <- as.data.frame(as.matrix(sapply(listVar, unlist)), stringsAsFactors = FALSE)
rownames(x) <- NULL
colnames(x) <- c("selection", "original.title", "english.title", "director")
x$Year <- year
return(x)
}
years <- 2013:1946
total <- length(years)
pb <- txtProgressBar(min = 0, max = total, style = 3)
cannesList <- list()
for(y in seq_len(total)){
cannesList[[y]] <- getCannesSelection(years[y])
setTxtProgressBar(pb, y)
}
close(pb)
cannesList
commentTable[,"content"] <- sapply(commentTable[,"content"],html2txt)
# Packages
# ========================================================================================================
library("rjson")
library("XML")
library("gsubfn")
# Functions
# ========================================================================================================
# from http://stackoverflow.com/questions/5060076/convert-html-character-entity-encoding-in-r
html2txt <- function(str) {
xpathApply(htmlParse(str, asText=TRUE),
"//body//text()",
xmlValue)[[1]]
}
getUrlsFromTopWeek <- function(urlTop){
topWeekHtml <- htmlParse(urlTop)
urls <- xpathSApply(topWeekHtml, "//*[@id='conteudo-principal']/section[*]/article[*]/p/a", xmlGetAttr, "href")
}
unlistNoEmpty <- function(someList){
selectEmpty <- sapply(someList, length) == 0
someList[selectEmpty] <- NA
unlist(someList)
}
tryReadJson <- function(jsonUrl, ntry =3){
readPageTryCount <- 0
while(readPageTryCount != 3){
jsonLines <- readLines(jsonUrl, warn = FALSE)
jsonString <- paste(jsonLines, collapse = "")
options("warn" = -1)     # To avoid last line warning from fromJSON
jsonData <- try(fromJSON(jsonString), silent = TRUE)
options("warn" = 0) # return to the norml warning behavior
if(class(jsonData) == "try-error"){
readPageTryCount <- readPageTryCount + 1
print(paste(readPageTryCount, "failed attempt to read X", jsonUrl))
} else {
readPageTryCount <- 3
}
}
return(jsonData)
}
getUolComment <- function(pagePatch){
pageHtlm <- readLines(pagePatch, warn = FALSE)
commentId <- strapplyc(pageHtlm, "Comentario=\\{\"id\":(\\d+)")[[1]]
# If is from Folha instead of UOL or is from a section that is not article
if(length(commentId) == 0){
return(NULL)
}
# we are not interest in the replies for now
commentPath <- paste("http://view.comentarios.uol.com.br/subject/", commentId, "?size=10000&load_replies=false", sep = "")
commentInfo <- tryReadJson(jsonUrl = commentPath)
# if there is no comments
if(length(commentInfo$docs) == 0){
return(NULL)
}
commentHeaders <- names(commentInfo$docs[[1]])
commentTable <- t(sapply(commentInfo$docs, unlistNoEmpty))
colnames(commentTable) <- commentHeaders
# The comments (they are in the 'content' field) need to be parse to text
commentTable[,"content"] <- sapply(commentTable[,"content"], html2txt)
return(commentTable)
}
# Get comments from the top articles of the last 30 weeks from UOL
# ========================================================================================================
url.indexTopWeekArticle <- "http://noticias.uol.com.br/top-da-semana/ultimas/"
html.indexTopWeekArticle <- htmlParse(url.indexTopWeekArticle)
url.topWeekArticles<- xpathSApply(html.indexTopWeekArticle, "//*[@id='conteudo-principal']/div[*]/section/section[*]/article/h1/a", xmlGetAttr, "href")
url.articles <- lapply(url.topWeekArticles, getUrlsFromTopWeek)
allTopList <- list()
countWeek <- 1
for(week in url.articles){
print(paste("WEEK:", countWeek))
listWeekComments <- list()
countArticles <- 1
for(article in week){
print(paste("ARTICLE:", countArticles))
listWeekComments[[countArticles]] <- getUolComment(article)
countArticles <- countArticles + 1
}
# remove the ones that no comment was retrieve
selectFolha <- sapply(listWeekComments, is.null)
listWeekComments[selectFolha] <- NULL
weekTable <- do.call(rbind, listWeekComments)
allTopList[[countWeek]] <- weekTable
countWeek <- countWeek + 1
}
allTopList[[1]]
setwd("~/Graficos4Fun/comentariosUOL/R/")
weekName <- gsub("^.*/|\\.htm", "", url.topWeekArticles)
weekNameFilesPath <- paste("../commentTables/", weekName, ".csv", sep = "")
names(allTopList) <- weekNameFilesPath
for(csv in names(allTopList)) write.csv(allTopList[[csv]], csv, row.names = FALSE)
Functions
# ========================================================================================================
splitInWords <- function(comment){
unlist(strsplit(comment, " +"))
}
nConsecutiveWords <- function(words, n, indexStart){
words[indexStart:(indexStart + n)]
}
allNConsecutiveWords <- function(words, n){
indexes <- head(seq_along(words), -2)
lapply(indexes, nConsecutiveWords, words = words, n = n)
}
stateAndNextWord <- function(setOfWords, nWordsByState){
t(sapply(setOfWords, function(x) c("state" = paste(x[1:nWordsByState], collapse = " "),
"nextWord" = x[nWordsByState + 1])))
}
wordsPairState <- function(words){
stateFirstWords <- head(words, - 1)
# When the comment have only one word
if(is.null(stateFirstWord)){
return(NULL)
}
stateSecondWords <- tail(words, -1)
states <- mapply(paste, stateFirstWords, stateSecondWords, USE.NAMES = FALSE)
# remove the last one (as it will don't have a word to go to)
states  <- head(states, -1)
return(states)
}
tablesFiles <- list.files("../commentTables/", pattern = "\\.csv", full.names = TRUE)
allWeeksTables <- lapply(tablesFiles, read.csv, stringsAsFactors = FALSE)
markovCommentInput <- allWeeksTables[[1]]
markovCommentInput[[1]]
markovCommentInput
comments <- commentTable$content
commentTable <- markovCommentInput[[1]]
lowerCases = FALSE
removeNewLine = FALSE
comments <- commentTable$content
comments <- commentTable[,"content"]
markovCommentInput <- allWeeksTables[[1]][[1]]
class(markovCommentInput)
commentTable <- markovCommentInput[[1]]
class(commentTable)
commentTable <- allWeeksTables[[1]]
class(commentTable)
comments <- commentTable["content"
]
comments <- commentTable$content
comments
if(removeNewLine){
comments <- gsub(pattern = "\\\\n", "", comments)
}
if(lowerCases){
comments <- tolower(comments)
}
grep("[[:punct:]][^[:blank:]]", comments)
comments[grep("[[:punct:]][^[:blank:]]", comments)]
library(stringr)
str_extract("[[:punct:]][^[:blank:]]", comments)
str_extract(pattern = "[[:punct:]][^[:blank:]]", comments)
str_extract(pattern = "[[:punct:]][^[:blank:][:punct:]]", comments)
str_extract(pattern = "[\\.\\,\\!\\:\\?][^[:blank:][:punct:]]", comments)
str_extract(pattern = "[\\.\\,\\!\\:\\?][^[:blank:]\\\\n[:punct:]]", comments)
str_extract(pattern = "[\\.\\,\\!\\:\\?][^[:blank:]\\\n[:punct:]]", comments)
str_extract(pattern = "[\\.\\,\\!\\:\\?\\-][^[:blank:]\\\n[:punct:]]", comments)
str_extract(pattern = "[\\.\\,\\!\\:\\?][^[:blank:]\\\n[:punct:]]", comments)
str_extract(pattern = "([\\.\\,\\!\\:\\?]+)([^[:blank:]), comments)
)
\q
str_extract(pattern = "([\\.\\,\\!\\:\\?]+)([^[:blank:])", comments)
str_extract(pattern = "([\\.\\,\\!\\:\\?]+)([^[:blank:]])", comments)
str_extract(pattern = "([\\.\\,\\!\\:\\?]+)([^[:blank:][\\.\\,\\!\\:\\?])", comments)
str_extract(pattern = "([\\.\\,\\!\\:\\?]+)([^[:blank:][\\.\\,\\!\\:\\?\\\n])", comments)
str_extract(pattern = "([\\.\\,\\!\\:\\?]+)([^[:blank:][:punct:]])", comments)
str_extract(pattern = "([\\.\\,\\!\\:\\?]+)([^[:blank:][:punct:][:space:]])", comments)
regexBadFormat <- "([\\.\\,\\!\\:\\?]+)([^[:blank:][:punct:][:space:]])"
regexBadFormat
gsub(pattern = regexBadFormat, replacement = "\1 \2", "lalalalalaalaal. wdeqwe, qweqeq.qwdqweqe")
gsub(pattern = regexBadFormat, replacement = "\\1 \\2", "lalalalalaalaal. wdeqwe, qweqeq.qwdqweqe")
regexBadFormat <- "([\\.\\,\\!\\:\\?]+)([^[:blank:][:punct:][:space:]])"
comments <- gsub(pattern = regexBadFormat, replacement = "\\1 \\2", comments)
x <- gsub("[\\(\\{\\[\"].*?[\\)\\}\\]\"]", "" , "adasdasdas (Asdsada) sadasdas")
x
x <- gsub("[\\(\\{\\[\"].*?[\\)\\}\\]\"]", "" , "adasdasdas (Asdsada) sadasdas")
x
x <- gsub("\\(.*?\\)", "" , "adasdasdas (Asdsada) sadasdas")
x
x <- gsub("[\\[\\(].*?[\\)\\]\", "" , "adasdasdas (Asdsada) sadasdas")
x <- gsub("[\\[\\(].*?[\\)\\]", "" , "adasdasdas (Asdsada) sadasdas")
x
x <- gsub("[\\[\\(].*?[\\)\\]]", "" , "adasdasdas (Asdsada) sadasdas")
x
x <- gsub("[\\[\\(].*?[\\)\]]", "" , "adasdasdas (Asdsada) sadasdas")
x <- gsub("\\(.*?\\)" , "" , "adasdasdas (Asdsada) sadasdas")
x
# Functions
# ========================================================================================================
splitInWords <- function(comment){
unlist(strsplit(comment, " +"))
}
nConsecutiveWords <- function(words, n, indexStart){
words[indexStart:(indexStart + n)]
}
allNConsecutiveWords <- function(words, n){
indexes <- head(seq_along(words), -2)
lapply(indexes, nConsecutiveWords, words = words, n = n)
}
stateAndNextWord <- function(setOfWords, nWordsByState){
t(sapply(setOfWords, function(x) c("state" = paste(x[1:nWordsByState], collapse = " "),
"nextWord" = x[nWordsByState + 1])))
}
wordsPairState <- function(words){
stateFirstWords <- head(words, - 1)
# When the comment have only one word
if(is.null(stateFirstWord)){
return(NULL)
}
stateSecondWords <- tail(words, -1)
states <- mapply(paste, stateFirstWords, stateSecondWords, USE.NAMES = FALSE)
# remove the last one (as it will don't have a word to go to)
states  <- head(states, -1)
return(states)
}
markovCommentInput <- function(commentTable, nWordsByState = 2, removeNewLine = FALSE, lowerCases = FALSE,
removeOpenCloseEstructures = TRUE # parenthesis, branckets, quotes.
){
comments <- commentTable$content
if(removeNewLine){
comments <- gsub(pattern = "\\\\n", "", comments)
}
if(lowerCases){
comments <- tolower(comments)
}
# There is a problem in balacing parenthesis, quotes, brackets, etc in the this type of program
# The solution I have seen is:
#  * remove the parentheses
#  * consider everything inside the parenthesis one word (you will probably end with the tree states from the same comment)
if(parenthesisOption == "remove"){
comments <- gsub("\\(.*?\\)" , "", comments)
comments <- gsub("\".*?\"" , "" , comments)
}
# Correct the comment that don't a space after a punctuation
regexBadFormat <- "([\\.\\,\\!\\:\\?]+)([^[:blank:][:punct:][:space:]])"
comments <- gsub(pattern = regexBadFormat, replacement = "\\1 \\2", comments)
words <- lapply(comments, splitInWords)
nWordsByComments <- sapply(indexWordsByComments, length)
words[nWordsByComments < (nWordsByState + 1)] <- NULL
consecutiveWords <- lapply(words, allNConsecutiveWords, n = nWordsByState)
stateNextWordList <- sapply(consecutiveWords, stateAndNextWord, nWordsByState = nWordsByState)
startStates <- sapply(stateNextWordList, function(x) x[1,1])
tableStateNextWordDF <- as.data.frame(do.call(rbind, stateNextWordList))
transitions <- split(as.character(tableStateNextWordDF$nextWord), tableStateNextWordDF$state)
return(list("startVector" = startStates, "transitionList" = transitions))
}
tablesFiles <- list.files("../commentTables/", pattern = "\\.csv", full.names = TRUE)
allWeeksTables <- lapply(tablesFiles, read.csv, stringsAsFactors = FALSE)
allcomments <- do.call(rbind, allWeeksTables)
markovsProp <- markovVarUol(allcomments)
markovsInp <- markovCommentInput(allcomments)
# Functions
# ========================================================================================================
splitInWords <- function(comment){
unlist(strsplit(comment, " +"))
}
nConsecutiveWords <- function(words, n, indexStart){
words[indexStart:(indexStart + n)]
}
allNConsecutiveWords <- function(words, n){
indexes <- head(seq_along(words), -2)
lapply(indexes, nConsecutiveWords, words = words, n = n)
}
stateAndNextWord <- function(setOfWords, nWordsByState){
t(sapply(setOfWords, function(x) c("state" = paste(x[1:nWordsByState], collapse = " "),
"nextWord" = x[nWordsByState + 1])))
}
wordsPairState <- function(words){
stateFirstWords <- head(words, - 1)
# When the comment have only one word
if(is.null(stateFirstWord)){
return(NULL)
}
stateSecondWords <- tail(words, -1)
states <- mapply(paste, stateFirstWords, stateSecondWords, USE.NAMES = FALSE)
# remove the last one (as it will don't have a word to go to)
states  <- head(states, -1)
return(states)
}
markovCommentInput <- function(commentTable, nWordsByState = 2, removeNewLine = FALSE, lowerCases = FALSE,
removeOpenCloseEstructures = TRUE # parenthesis, branckets, quotes.
){
comments <- commentTable$content
if(removeNewLine){
comments <- gsub(pattern = "\\\\n", "", comments)
}
if(lowerCases){
comments <- tolower(comments)
}
# There is a problem in balacing parenthesis, quotes, brackets, etc in the this type of program
# The solution I have seen is:
#  * remove the parentheses
#  * consider everything inside the parenthesis one word (you will probably end with the tree states from the same comment)
if(removeOpenCloseEstructures){
comments <- gsub("\\(.*?\\)" , "", comments)
comments <- gsub("\".*?\"" , "" , comments)
}
# Correct the comment that don't a space after a punctuation
regexBadFormat <- "([\\.\\,\\!\\:\\?]+)([^[:blank:][:punct:][:space:]])"
comments <- gsub(pattern = regexBadFormat, replacement = "\\1 \\2", comments)
words <- lapply(comments, splitInWords)
nWordsByComments <- sapply(indexWordsByComments, length)
words[nWordsByComments < (nWordsByState + 1)] <- NULL
consecutiveWords <- lapply(words, allNConsecutiveWords, n = nWordsByState)
stateNextWordList <- sapply(consecutiveWords, stateAndNextWord, nWordsByState = nWordsByState)
startStates <- sapply(stateNextWordList, function(x) x[1,1])
tableStateNextWordDF <- as.data.frame(do.call(rbind, stateNextWordList))
transitions <- split(as.character(tableStateNextWordDF$nextWord), tableStateNextWordDF$state)
return(list("startVector" = startStates, "transitionList" = transitions))
}
# Make ther markov variables for each week and for all weeks
# ========================================================================================================
tablesFiles <- list.files("../commentTables/", pattern = "\\.csv", full.names = TRUE)
allWeeksTables <- lapply(tablesFiles, read.csv, stringsAsFactors = FALSE)
allcomments <- do.call(rbind, allWeeksTables)
markovsInp <- markovCommentInput(allcomments)
# Functions
# ========================================================================================================
splitInWords <- function(comment){
unlist(strsplit(comment, " +"))
}
nConsecutiveWords <- function(words, n, indexStart){
words[indexStart:(indexStart + n)]
}
allNConsecutiveWords <- function(words, n){
indexes <- head(seq_along(words), -2)
lapply(indexes, nConsecutiveWords, words = words, n = n)
}
stateAndNextWord <- function(setOfWords, nWordsByState){
t(sapply(setOfWords, function(x) c("state" = paste(x[1:nWordsByState], collapse = " "),
"nextWord" = x[nWordsByState + 1])))
}
wordsPairState <- function(words){
stateFirstWords <- head(words, - 1)
# When the comment have only one word
if(is.null(stateFirstWord)){
return(NULL)
}
stateSecondWords <- tail(words, -1)
states <- mapply(paste, stateFirstWords, stateSecondWords, USE.NAMES = FALSE)
# remove the last one (as it will don't have a word to go to)
states  <- head(states, -1)
return(states)
}
markovCommentInput <- function(commentTable, nWordsByState = 2, removeNewLine = FALSE, lowerCases = FALSE,
removeOpenCloseEstructures = TRUE # parenthesis, branckets, quotes.
){
comments <- commentTable$content
if(removeNewLine){
comments <- gsub(pattern = "\\\\n", "", comments)
}
if(lowerCases){
comments <- tolower(comments)
}
# There is a problem in balacing parenthesis, quotes, brackets, etc in the this type of program
# The solution I have seen is:
#  * remove the parentheses
#  * consider everything inside the parenthesis one word (you will probably end with the tree states from the same comment)
if(removeOpenCloseEstructures){
comments <- gsub("\\(.*?\\)" , "", comments)
comments <- gsub("\".*?\"" , "" , comments)
}
# Correct the comment that don't a space after a punctuation
regexBadFormat <- "([\\.\\,\\!\\:\\?]+)([^[:blank:][:punct:][:space:]])"
comments <- gsub(pattern = regexBadFormat, replacement = "\\1 \\2", comments)
words <- lapply(comments, splitInWords)
words[nWordsByComments < (nWordsByState + 1)] <- NULL
consecutiveWords <- lapply(words, allNConsecutiveWords, n = nWordsByState)
stateNextWordList <- sapply(consecutiveWords, stateAndNextWord, nWordsByState = nWordsByState)
startStates <- sapply(stateNextWordList, function(x) x[1,1])
tableStateNextWordDF <- as.data.frame(do.call(rbind, stateNextWordList))
transitions <- split(as.character(tableStateNextWordDF$nextWord), tableStateNextWordDF$state)
return(list("startVector" = startStates, "transitionList" = transitions))
}
# Make ther markov variables for each week and for all weeks
# ========================================================================================================
tablesFiles <- list.files("../commentTables/", pattern = "\\.csv", full.names = TRUE)
allWeeksTables <- lapply(tablesFiles, read.csv, stringsAsFactors = FALSE)
allcomments <- do.call(rbind, allWeeksTables)
markovsInp <- markovCommentInput(allcomments)
# Functions
# ========================================================================================================
splitInWords <- function(comment){
unlist(strsplit(comment, " +"))
}
nConsecutiveWords <- function(words, n, indexStart){
words[indexStart:(indexStart + n)]
}
allNConsecutiveWords <- function(words, n){
indexes <- head(seq_along(words), -2)
lapply(indexes, nConsecutiveWords, words = words, n = n)
}
stateAndNextWord <- function(setOfWords, nWordsByState){
t(sapply(setOfWords, function(x) c("state" = paste(x[1:nWordsByState], collapse = " "),
"nextWord" = x[nWordsByState + 1])))
}
wordsPairState <- function(words){
stateFirstWords <- head(words, - 1)
# When the comment have only one word
if(is.null(stateFirstWord)){
return(NULL)
}
stateSecondWords <- tail(words, -1)
states <- mapply(paste, stateFirstWords, stateSecondWords, USE.NAMES = FALSE)
# remove the last one (as it will don't have a word to go to)
states  <- head(states, -1)
return(states)
}
markovCommentInput <- function(commentTable, nWordsByState = 2, removeNewLine = FALSE, lowerCases = FALSE,
removeOpenCloseEstructures = TRUE # parenthesis, branckets, quotes.
){
comments <- commentTable$content
if(removeNewLine){
comments <- gsub(pattern = "\\\\n", "", comments)
}
if(lowerCases){
comments <- tolower(comments)
}
# There is a problem in balacing parenthesis, quotes, brackets, etc in the this type of program
# The solution I have seen is:
#  * remove the parentheses
#  * consider everything inside the parenthesis one word (you will probably end with the tree states from the same comment)
if(removeOpenCloseEstructures){
comments <- gsub("\\(.*?\\)" , "", comments)
comments <- gsub("\".*?\"" , "" , comments)
}
# Correct the comment that don't a space after a punctuation
regexBadFormat <- "([\\.\\,\\!\\:\\?]+)([^[:blank:][:punct:][:space:]])"
comments <- gsub(pattern = regexBadFormat, replacement = "\\1 \\2", comments)
words <- lapply(comments, splitInWords)
nWordsByComments <- sapply(words, length)
words[nWordsByComments < (nWordsByState + 1)] <- NULL
consecutiveWords <- lapply(words, allNConsecutiveWords, n = nWordsByState)
stateNextWordList <- sapply(consecutiveWords, stateAndNextWord, nWordsByState = nWordsByState)
startStates <- sapply(stateNextWordList, function(x) x[1,1])
tableStateNextWordDF <- as.data.frame(do.call(rbind, stateNextWordList))
transitions <- split(as.character(tableStateNextWordDF$nextWord), tableStateNextWordDF$state)
return(list("startVector" = startStates, "transitionList" = transitions))
}
# Make ther markov variables for each week and for all weeks
# ========================================================================================================
tablesFiles <- list.files("../commentTables/", pattern = "\\.csv", full.names = TRUE)
allWeeksTables <- lapply(tablesFiles, read.csv, stringsAsFactors = FALSE)
allcomments <- do.call(rbind, allWeeksTables)
markovsInp <- markovCommentInput(allcomments)
markovsProp <-markovsInp
statePresent <- sample(x = markovsProp$startVector, size = 1)
head(statePresent)
statePresent
class(markovsInp)
names(markovsInp)
class(markovsInp$transitionList)
markovsInp$transitionList[[1]]
